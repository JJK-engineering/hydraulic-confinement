{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# python setup for qgis processing\n",
    "\n",
    "import sys\n",
    "from qgis.core import *\n",
    "# need qgis,gui ?\n",
    "#from qgis.gui import *\n",
    "# need PyQt4.QtCore ?\n",
    "#from PyQt4.QtCore import *\n",
    "from PyQt4.QtGui import *\n",
    "\n",
    "# what does True refer to below ?\n",
    "app = QApplication([], True)\n",
    "QgsApplication.setPrefixPath(\"/usr\", True)\n",
    "# /usr correct?\n",
    "#QgsApplication.setPrefixPath(qgis_path, True)\n",
    "QgsApplication.initQgis()\n",
    "\n",
    "sys.path.append('/usr/share/qgis/python/plugins')\n",
    "from processing.core.Processing import Processing\n",
    "Processing.initialize()\n",
    "from processing.tools import *\n",
    "\n",
    "#why is this still needed\n",
    "import processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# ----------------------------------------------------------------------------------------------------------------\n",
    "# description\n",
    "# ----------------------------------------------------------------------------------------------------------------\n",
    "\n",
    "#!/usr/bin/python\n",
    "# TunnelExcavationData.py\n",
    "\n",
    "# Python procedure for TunnelGIS Engineering App\n",
    "# Author: KK\n",
    "# Date: 01.04.2017\n",
    "\n",
    "# Purpose of this procedure:\n",
    "# 1. Prepare input data ....\n",
    "# 2.\n",
    "# 3.\n",
    "# 4. \n",
    "# 5. \n",
    "\n",
    "# This python routine is a script, intended to guide the user through the described procedure.\n",
    "# As a script, the procedure does not generally include data validation and error handling.\n",
    "# Users are expected to understand and adjust the code as needed for their application.\n",
    "\n",
    "# Required Input Files:\n",
    "#   \"WORK/swissalti3dgeotifflv03-5m/swissALTI3D_.tif\"   -DEM with surface topography\n",
    "#   \"WORK/Felsisohypsen-raster.tif\"                     -DEM with rock surface\n",
    "#   \"WORK/OstrohrR2.csv\"                                -stationed tunnel alignment#\n",
    "#   \"WORK/Ostroehre.TunnelLayoutData.R2.csv\"            -tunnel layout data\n",
    "\n",
    "# References:\n",
    "# http://gis.stackexchange.com/questions/197825/how-to-convert-multiple-csv-files-to-shp-using-python-and-no-arcpy\n",
    "# to get grass help:   processing.alghelp(\"grass7:r.what.points\")\n",
    "\n",
    "# IMPORTANT: requires qgis setup before running this procedure\n",
    "# run ./pyqgis.sh from command line before starting python (or set up IDE accordingly)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# ----------------------------------------------------------------------------------------------------------------\n",
    "# import required libraries\n",
    "# ----------------------------------------------------------------------------------------------------------------\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import geopandas as gpd\n",
    "import shapely as sp\n",
    "import os "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# ----------------------------------------------------------------------------------------------------------------\n",
    "# set wd for this procedure \n",
    "# ----------------------------------------------------------------------------------------------------------------\n",
    "os.chdir(\"/home/kaelin_joseph/DSS.HydraulicConfinement/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# ----------------------------------------------------------------------------------------------------------------\n",
    "# define input files\n",
    "# ----------------------------------------------------------------------------------------------------------------\n",
    "\n",
    "DTM = \"WORK/swissalti3dgeotifflv03-5m/swissALTI3D_.tif\"  \n",
    "RockSurface = \"WORK/Felsisohypsen-raster.tif\"            \n",
    "AlignmentData = \"WORK/Ostroehre.AlignmentData.R2.csv\"\n",
    "LayoutData = \"WORK/Ostroehre.TunnelLayoutData.R2.csv\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# ----------------------------------------------------------------------------------------------------------------\n",
    "# define Bore Classes\n",
    "#   define Bore Classes as class, to separate definition of methods from execution\n",
    "#   class method is used as a modifier to the TunnelExcavationData (dataframe) class.\n",
    "# ----------------------------------------------------------------------------------------------------------------\n",
    "# better to add mtethods .bc1, .bc2, .bc3 to TunnExcvDf ??\n",
    "\n",
    "tunn_h =13.0   # define tunnel height\n",
    "volume_unit='m3'  # unit to be used for volume calculation and reporting\n",
    "\n",
    "class BoreClass:\n",
    "    \"\"\"Determine Bore Class for TBM tunnels\"\"\"\n",
    "    # BC1 - tunnel predominantly in soil\n",
    "    def bc1(self):\n",
    "        TunnExcvDF.loc[(TunnExcvDF[\"ExcavationType\"] == \"TBM\") & \n",
    "        (TunnExcvDF[\"RockSurface\"] <= TunnExcvDF[\"Elevation\"] -tunn_h*0.25),\"BoreClass\"] \\\n",
    "        =\"BC1\"\n",
    "    # BC2 - tunnel with mixed face\n",
    "    def bc2(self):\n",
    "        TunnExcvDF.loc[(TunnExcvDF[\"ExcavationType\"] == \"TBM\") & \n",
    "        (TunnExcvDF[\"RockSurface\"] > TunnExcvDF[\"Elevation\"] -tunn_h*0.25) & \n",
    "        (TunnExcvDF[\"RockSurface\"] < TunnExcvDF[\"Elevation\"] +tunn_h/2.0 +1.5),\"BoreClass\"] \\\n",
    "        = \"BC2\"\n",
    "    # BC3 - tunnel inf rock\n",
    "    def bc3(self):\n",
    "        TunnExcvDF.loc[(TunnExcvDF[\"ExcavationType\"] == \"TBM\") & \\\n",
    "        (TunnExcvDF[\"RockSurface\"] >= TunnExcvDF[\"Elevation\"] +tunn_h/2.0 +1.5),\"BoreClass\"] \\\n",
    "        = \"BC3\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# ----------------------------------------------------------------------------------------------------------------\n",
    "# define output files\n",
    "# ----------------------------------------------------------------------------------------------------------------\n",
    "\n",
    "TunnelExcavationData = \"WORK/Ostroehre.TunnelExcavationData.R2.csv\"\n",
    "# headers: Station, Easting, Northing, Elevation, DTM, RockSurface, StationReal, RockCover,\n",
    "#          WBScode, WorkType, ExcavationType, ProfileType, SectionArea, Description,\n",
    "#          BoreClass, SupportClass, DisposalClass, StationInterval, ExcavationVolume, DisposalVolume\n",
    "Alignment_SHP ='WORK/Ostroehre.Alignment.R2.shp'\n",
    "BoQ = \"WORK/Ostroehre.TunnelBoQdata.R2.csv\"\n",
    "# temporary data\n",
    "Alignment_DTM = \"WORK/Ostroehre.Terrain.R2.csv\"\n",
    "Alignment_RockSurface = \"WORK/Ostroehre.RockSurface.R2.csv\"  # JK ToDo: RockSurface?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# ----------------------------------------------------------------------------------------------------------------\n",
    "# create alignment_df (dataframe) from .csv\n",
    "# Important: Before the df is created the data should be checked.\n",
    "#   E.g. make sure that it does not contain trailing blank lines and that duplicate lines are deleted.\n",
    "# result: alignment_df\n",
    "# ----------------------------------------------------------------------------------------------------------------\n",
    "\n",
    "alignment_df = pd.read_csv(AlignmentData)\n",
    "#delete row if only NA are present in row\n",
    "alignment_df = alignment_df.dropna(how = \"all\")\n",
    "# round alignment_df to three decimals\n",
    "alignment_df = alignment_df.round(decimals=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#alignment_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# ----------------------------------------------------------------------------------------------------------------\n",
    "# create layout_df from .csv\n",
    "# result: layout_df\n",
    "# ----------------------------------------------------------------------------------------------------------------\n",
    "\n",
    "layout_df = pd.read_csv(LayoutData)\n",
    "# round layout_df to three decimals\n",
    "layout_df = layout_df.round(decimals=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#layout_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "creating df header StationReal\n"
     ]
    }
   ],
   "source": [
    "# ----------------------------------------------------------------------------------------------------------------\n",
    "# convert alignment_df[\"Station\"] => alignment_df[\"StationReal\"] and similar for layout_df\n",
    "# result: alignment_Station_list, layout_Station_list\n",
    "#         alignment_df[\"StationReal\"], layout_df[\"StationReal\"]\n",
    "print \"creating df header StationReal\"\n",
    "# ----------------------------------------------------------------------------------------------------------------\n",
    "\n",
    "alignment_Station_list = alignment_df[\"Station\"].tolist()\n",
    "    # check: len(alignment_Station_list)\n",
    "\n",
    "alignment_df[\"StationReal\"] = np.nan\n",
    "\n",
    "for n in range(0, len(alignment_Station_list)):\n",
    "    station_sel = alignment_df.iloc[n][\"Station\"]\n",
    "    station_real_sel = float(station_sel.replace(\"+\",\"\"))\n",
    "    alignment_df.iloc[n, alignment_df.columns.get_loc(\"StationReal\")] = station_real_sel\n",
    "    # alignment_df.columns.get_loc(\"StationReal\") = 5\n",
    "\n",
    "# layout_df[\"Station\"] => layout_df[\"StationReal\"] \n",
    "layout_Station_list = layout_df[\"Station\"].tolist()\n",
    "    # check: len(layout_Station_list)\n",
    "\n",
    "layout_df[\"StationReal\"] = np.nan\n",
    "\n",
    "for n in range(0, len(layout_Station_list)):\n",
    "    station_sel = layout_df.iloc[n][\"Station\"]\n",
    "    station_real_sel = float(station_sel.replace(\"+\",\"\"))\n",
    "    layout_df.iloc[n, layout_df.columns.get_loc(\"StationReal\")] \\\n",
    "        = station_real_sel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "adding Stations\n",
      "            Point Type  Station     Northing      Easting  Elevation  StationReal\n",
      "427  205+488  NaN  205+488  1268944.416  2612475.076    248.149     205488.0\n",
      "            Point Type  Station     Northing      Easting  Elevation  StationReal\n",
      "429  205+491  NaN  205+491  1268941.813  2612476.566    247.999     205491.0\n",
      "    station_real 205490.0\n",
      "    easting_newpoint_sel 2612476.06957\n",
      "    northing_newpoint_sel 1268942.68025\n",
      "    elevation_newpoint_sel 248.048976153\n",
      "            Point Type  Station     Northing      Easting  Elevation  StationReal\n",
      "447  205+518  NaN  205+518  1268918.379  2612489.977    246.648     205518.0\n",
      "            Point Type  Station     Northing      Easting  Elevation  StationReal\n",
      "449  205+521  NaN  205+521  1268915.775  2612491.468    246.498     205521.0\n",
      "    station_real 205520.0\n",
      "    easting_newpoint_sel 2612490.97078\n",
      "    northing_newpoint_sel 1268916.64338\n",
      "    elevation_newpoint_sel 246.548021643\n"
     ]
    }
   ],
   "source": [
    "# ----------------------------------------------------------------------------------------------------------------\n",
    "# check if every layout_df[\"StationReal\"] exists in alignment_df[\"StationReal\"]\n",
    "#   If it does not exist, create a new Station in alignment_df\n",
    "# result: alignment_StationReal_list, layout_StationReal_list\n",
    "#         alignment_df with added Stations\n",
    "print \"adding Stations\"\n",
    "# ----------------------------------------------------------------------------------------------------------------\n",
    "\n",
    "alignment_StationReal_list = alignment_df[\"StationReal\"].tolist()\n",
    "layout_StationReal_list = layout_df[\"StationReal\"].tolist()\n",
    "\n",
    "# within this vicinity of a station no new station will be created\n",
    "vicinity = 0.1\n",
    "\n",
    "# define new variables\n",
    "easting_newpoint = []\n",
    "northing_newpoint = []\n",
    "elevation_newpoint = []\n",
    "station = []\n",
    "station_real = []\n",
    "\n",
    "# Loop through stations\n",
    "for n in layout_StationReal_list:\n",
    "    if n in alignment_StationReal_list:\n",
    "        pass\n",
    "    else:\n",
    "        neighbour1_StationReal = max([i for i in alignment_StationReal_list if i < n]) \n",
    "        neighbour2_StationReal = min([i for i in alignment_StationReal_list if i > n])       \n",
    "        ####if n < neighbour1_StationReal + vicinity or n > neighbour2_StationReal + vicinity:      KLK: check\n",
    "        if n < neighbour1_StationReal + vicinity or n > neighbour2_StationReal - vicinity:\n",
    "            pass\n",
    "        else: \n",
    "            neighbour1 = alignment_df.loc[alignment_df[\"StationReal\"]\n",
    "                                          == neighbour1_StationReal,]\n",
    "            neighbour2 = alignment_df.loc[alignment_df[\"StationReal\"]\n",
    "                                          == neighbour2_StationReal,]\n",
    "            ####delta_x_neighbour1_2 = abs(neighbour2.Easting.tolist()[0] -neighbour1.Easting.tolist()[0])\n",
    "            ####delta_y_neighbour1_2 = abs(neighbour2.Northing.tolist()[0] -neighbour1.Northing.tolist()[0])\n",
    "            ####                                                                                  KLK: check\n",
    "            delta_x_neighbour1_2 = neighbour2.Easting.tolist()[0] -neighbour1.Easting.tolist()[0] #delta x\n",
    "            delta_y_neighbour1_2 = neighbour2.Northing.tolist()[0] -neighbour1.Northing.tolist()[0] #delta y\n",
    "            delta_z_neighbour1_2 = neighbour2.Elevation.tolist()[0] -neighbour1.Elevation.tolist()[0] #delta z\n",
    "            length_neighbour1_2 = (delta_x_neighbour1_2**2 +delta_y_neighbour1_2**2)**(0.5) # L\n",
    "            length_neighbour1_newpoint = n- neighbour1.StationReal.tolist()[0]\n",
    "            ####easting_newpoint_sel = neighbour2.Easting.tolist()[0] \\\n",
    "            ####                     +((delta_y_neighbour1_2*length_neighbour1_newpoint)/length_neighbour1_2)\n",
    "            ####northing_newpoint_sel = neighbour2.Northing.tolist()[0] \\\n",
    "            ####                      +((delta_x_neighbour1_2*length_neighbour1_newpoint)/length_neighbour1_2)\n",
    "            ####elevation_newpoint_sel = neighbour2.Elevation.tolist()[0] \\\n",
    "            ####                       +((delta_x_neighbour1_2*length_neighbour1_newpoint)/length_neighbour1_2)\n",
    "            ####                                                                                  KLK: check\n",
    "            easting_newpoint_sel = neighbour1.Easting.tolist()[0] \\\n",
    "                                 +((delta_x_neighbour1_2*length_neighbour1_newpoint)/length_neighbour1_2)\n",
    "            northing_newpoint_sel = neighbour1.Northing.tolist()[0] \\\n",
    "                                  +((delta_y_neighbour1_2*length_neighbour1_newpoint)/length_neighbour1_2)\n",
    "            elevation_newpoint_sel = neighbour1.Elevation.tolist()[0] \\\n",
    "                                   +((delta_z_neighbour1_2*length_neighbour1_newpoint)/length_neighbour1_2)\n",
    "            easting_newpoint.append(easting_newpoint_sel)\n",
    "            northing_newpoint.append(northing_newpoint_sel)\n",
    "            elevation_newpoint.append(elevation_newpoint_sel)\n",
    "            station_real.append(n)\n",
    "            print \"    \", neighbour1\n",
    "            print \"    \", neighbour2\n",
    "            print \"    station_real\", n\n",
    "            print \"    easting_newpoint_sel\", easting_newpoint_sel\n",
    "            print \"    northing_newpoint_sel\", northing_newpoint_sel\n",
    "            print \"    elevation_newpoint_sel\", elevation_newpoint_sel\n",
    "            # this procedure must be tested for all combinations of ascending/descending\n",
    "            #   Northing, Easting and Elevation --> should be OK\n",
    "            #   for descending Stationing --> needs fixing                                          JK ToDo\n",
    "            Station_sel = layout_df.loc[layout_df['StationReal']\n",
    "                                                  == n, 'Station'] \n",
    "            station.append(Station_sel.tolist()[0])\n",
    "           \n",
    "newStation_df = pd.DataFrame({\"Easting\": easting_newpoint, \"Northing\": northing_newpoint,\n",
    "                              \"Elevation\": elevation_newpoint, \"StationReal\": station_real,\n",
    "                              \"Station\": station})\n",
    "    # check len(alignment_df)\n",
    "    # check len(newStation_df)\n",
    "\n",
    "# Contatenate alignment_df with newStation_df\n",
    "# result: alignment_df\n",
    "frames = [alignment_df, newStation_df]\n",
    "alignment_df = pd.concat(frames)\n",
    "    # check: len(alignment_df)\n",
    "    # check: alignment_df.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# ----------------------------------------------------------------------------------------------------------------\n",
    "# create Alignment_spatial from alignment_df\n",
    "# result: Alignment_SHP\n",
    "# ----------------------------------------------------------------------------------------------------------------\n",
    "\n",
    "Alignment_spatial_points = [sp.geometry.Point(row['Easting'], row['Northing'])\n",
    "                            for key, row in alignment_df.iterrows()]\n",
    "Alignment_crs = {'init': 'epsg:2056'}  #define crs\n",
    "Alignment_spatial = gpd.GeoDataFrame(alignment_df, geometry=Alignment_spatial_points, crs = Alignment_crs)\n",
    "Alignment_spatial.to_file(Alignment_SHP, driver='ESRI Shapefile') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "get raster values\n",
      "Warning: Not all input layers use the same CRS.\n",
      "This can cause unexpected results.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'output': 'WORK/Ostroehre.RockSurface.R2.csv'}"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# ----------------------------------------------------------------------------------------------------------------\n",
    "# use grass functions to get raster values for points along tunnel axis and write to .csv files\n",
    "# result: Alignment_DTM, Alignment_RockSurface\n",
    "print \"get raster values\"\n",
    "# ----------------------------------------------------------------------------------------------------------------\n",
    "\n",
    "# Alignment_DTM\n",
    "processing.runalg(\"grass7:r.what.points\",DTM,Alignment_SHP,\n",
    "                  \"NA\",\",\",500,True,False,False,False,False,\n",
    "                  \"2603510.0,2624270.0,1260650.0,1274890.0\",-1,0.0001,Alignment_DTM)\n",
    "# Alignment_RockSurface                  \n",
    "processing.runalg(\"grass7:r.what.points\",RockSurface,Alignment_SHP,\n",
    "                  \"NA\",\",\",500, True,False,False,False,False,\n",
    "                  \"2603510.0,2624270.0,1260650.0,1274890.0\",-1,0.0001,Alignment_RockSurface)\n",
    "## warning: Not all input layers use the same CRS -> data seems OK\n",
    "    # check:  Alginemnt_spatial.crs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# ----------------------------------------------------------------------------------------------------------------\n",
    "# create df's\n",
    "# result: Alignment_DTM_df, Alignment_RockSurface_df\n",
    "# ----------------------------------------------------------------------------------------------------------------\n",
    "Alignment_DTM_df  = pd.read_csv(Alignment_DTM)\n",
    "Alignment_RockSurface_df  = pd.read_csv(Alignment_RockSurface)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Alignment_DTM_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ----------------------------------------------------------------------------------------------------------------\n",
    "# prepare for join of grass results using pandas\n",
    "# result: alignment_df, Alignment_DTM_df_sel, Alignment_RockSurface_df_sel\n",
    "# ----------------------------------------------------------------------------------------------------------------\n",
    "\n",
    "# prepare alignment_df\n",
    "    # check:  alignment_df.head()\n",
    "alignment_df = alignment_df.loc[:,[\"Station\",\"Easting\", \"Northing\", \"Elevation\", \"StationReal\"]]\n",
    "    # check:  alignment_df.head()\n",
    "\n",
    "# prepare Alignment_DTM_df_sel\n",
    "    # check:  Alignment_DTM_df.head()\n",
    "Alignment_DTM_df_coleqtmp = [col for col in Alignment_DTM_df.columns if 'tmp' in col]\n",
    "if len(Alignment_DTM_df_coleqtmp) != 1:\n",
    "    print \"Extraction of DTM col=tmp did not work properly. Please check\"\n",
    "    exit()\n",
    "Alignment_DTM_df_rename = Alignment_DTM_df.rename(\n",
    "    columns= {Alignment_DTM_df_coleqtmp[0]: \"DTM\"})\n",
    "Alignment_DTM_df_sel = Alignment_DTM_df_rename.loc[:,[\"easting\", \"northing\", \"DTM\"]]\n",
    "    # check:  Alignment_RockSurface_df.head()\n",
    "\n",
    "# prepare Alignment_RockSurface_df_coleqtmp\n",
    "Alignment_RockSurface_df_coleqtmp = [col for col in Alignment_RockSurface_df.columns if 'tmp' in col]\n",
    "if len(Alignment_RockSurface_df_coleqtmp) != 1:\n",
    "    print \"Extraction of RockSurface_csv_coleqtmp col=tmp did not work properly. Please check\"\n",
    "    exit()\n",
    "Alignment_RockSurface_df_rename = Alignment_RockSurface_df.rename(\n",
    "    columns= {Alignment_RockSurface_df_coleqtmp[0]: \"RockSurface\"})\n",
    "Alignment_RockSurface_df_sel = Alignment_RockSurface_df_rename.loc[\n",
    "    :,[\"easting\", \"northing\", \"RockSurface\"]]  \n",
    "    # check:  Alignment_RockSurface_df_sel.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "merge_final\n"
     ]
    }
   ],
   "source": [
    "# ----------------------------------------------------------------------------------------------------------------\n",
    "# join grass results using Panda\n",
    "#    merge handles floats as keys inconsistently, round df's to three decimals before merge \n",
    "# result: merge_final\n",
    "print 'merge_final'\n",
    "# ----------------------------------------------------------------------------------------------------------------\n",
    "\n",
    "alignment_df = alignment_df.round(decimals=3)\n",
    "Alignment_DTM_df_sel = Alignment_DTM_df_sel.round(decimals=3)\n",
    "Alignment_RockSurface_df_sel = Alignment_RockSurface_df_sel.round(decimals=3)\n",
    "\n",
    "# merge DTM to Alignment\n",
    "merge_Alignment_DTM= pd.merge(left= alignment_df, right = Alignment_DTM_df_sel, \n",
    "                 left_on = [\"Easting\",\"Northing\"], \n",
    "                 right_on = [\"easting\",\"northing\"], how = \"left\")\n",
    "\n",
    "# merge RockSurface to Alignment_DTM\n",
    "merge_final = pd.merge(merge_Alignment_DTM, Alignment_RockSurface_df_sel, \n",
    "                 left_on = [\"Easting\",\"Northing\"], \n",
    "                 right_on = [\"easting\",\"northing\"])\n",
    "    # check:  merge_final.head()\n",
    "    # check:  merge_final.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cleaning up merge\n"
     ]
    }
   ],
   "source": [
    "# ----------------------------------------------------------------------------------------------------------------\n",
    "# clean up merge_final\n",
    "# result: TunnExcvDF\n",
    "print 'cleaning up merge'\n",
    "# ----------------------------------------------------------------------------------------------------------------\n",
    "\n",
    "TunnExcvDF = merge_final.loc[:,[\"Station\",\"Easting\", \"Northing\", \"Elevation\", \"DTM\", \"RockSurface\",\n",
    "                               \"StationReal\"]]\n",
    "    # check:  TunnExcvDF.head()\n",
    "    # check:  list(TunnExcvDF)\n",
    "# sort by Station\n",
    "TunnExcvDF = TunnExcvDF.sort(['StationReal'], ascending=[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# ----------------------------------------------------------------------------------------------------------------\n",
    "# calculate difference height rocksurface and tunnel axis\n",
    "# result: TunnExcvDF['RockCover']\n",
    "# ----------------------------------------------------------------------------------------------------------------\n",
    "\n",
    "TunnExcvDF['RockCover'] = TunnExcvDF.RockSurface - TunnExcvDF.Elevation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "working on WBS etc\n"
     ]
    }
   ],
   "source": [
    "# ----------------------------------------------------------------------------------------------------------------\n",
    "# assign WBS, WorkType, Excavation Type, Profile Type, Section Area from TunnelLayoutDarta\n",
    "# result: TunnExcvDF[\"WBScode\"], TunnExcvDF[\"WorkType\"], TunnExcvDF[\"ExcavationType\"], TunnExcvDF[\"ProfileType\"]\n",
    "#         TunnExcvDF[\"SectionArea\"], TunnExcvDF[\"Description\"]\n",
    "print \"working on WBS etc\"\n",
    "# ----------------------------------------------------------------------------------------------------------------\n",
    "\n",
    "TunnExcvDF[\"WBScode\"] = np.nan\n",
    "TunnExcvDF[\"WorkType\"] = np.nan\n",
    "TunnExcvDF[\"ExcavationType\"] = np.nan\n",
    "TunnExcvDF[\"ProfileType\"] = np.nan\n",
    "TunnExcvDF[\"SectionArea\"] = np.nan\n",
    "TunnExcvDF[\"Description\"] = np.nan\n",
    "TunnExcvDF[\"Unit\"] = volume_unit\n",
    "\n",
    "for n in range(0, len(layout_StationReal_list)):\n",
    "    nn = n+1 \n",
    "    if n == len(layout_StationReal_list) -1:\n",
    "        layout_StationReal_list.append(1e12)\n",
    "    TunnExcvDF.loc[(TunnExcvDF[\"StationReal\"] >= layout_StationReal_list[n])\n",
    "        & (TunnExcvDF[\"StationReal\"] < layout_StationReal_list[nn]), \"WBScode\"] \\\n",
    "        = layout_df[\"WBScode\"].tolist()[n]\n",
    "    TunnExcvDF.loc[(TunnExcvDF[\"StationReal\"] >= layout_StationReal_list[n])\n",
    "        & (TunnExcvDF[\"StationReal\"] < layout_StationReal_list[nn]), \"WorkType\"] \\\n",
    "        = layout_df[\"WorkType\"].tolist()[n]\n",
    "    TunnExcvDF.loc[(TunnExcvDF[\"StationReal\"] >= layout_StationReal_list[n])\n",
    "        & (TunnExcvDF[\"StationReal\"] < layout_StationReal_list[nn]), \"ExcavationType\"] \\\n",
    "        = layout_df[\"ExcavationType\"].tolist()[n]\n",
    "    TunnExcvDF.loc[(TunnExcvDF[\"StationReal\"] >= layout_StationReal_list[n])\n",
    "        & (TunnExcvDF[\"StationReal\"] < layout_StationReal_list[nn]), \"ProfileType\"] \\\n",
    "        = layout_df[\"ProfileType\"].tolist()[n]\n",
    "    TunnExcvDF.loc[(TunnExcvDF[\"StationReal\"] >= layout_StationReal_list[n])\n",
    "        & (TunnExcvDF[\"StationReal\"] < layout_StationReal_list[nn]), \"SectionArea\"] \\\n",
    "        = layout_df[\"SectionArea\"].tolist()[n]\n",
    "    TunnExcvDF.loc[(TunnExcvDF[\"StationReal\"] >= layout_StationReal_list[n])\n",
    "        & (TunnExcvDF[\"StationReal\"] < layout_StationReal_list[nn]), \"Description\"] \\\n",
    "        = layout_df[\"Description\"].tolist()[n]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "calculating BoreClass, SupportClass and DisposalClass\n",
      "BC3    805\n",
      "BC2    188\n",
      "BC1     60\n",
      "Name: BoreClass, dtype: int64\n",
      "TBM    1053\n",
      "MUL      51\n",
      "Name: ExcavationType, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# ----------------------------------------------------------------------------------------------------------------\n",
    "# calculate \"BoreClass\", \"SupportClass\" and \"DisposalClass\"\n",
    "# result: TunnExcvDF[\"BoreClass\"], TunnExcvDF[\"SupportClass\"], TunnExcvDF[\"DisposalClass\"]\n",
    "print 'calculating BoreClass, SupportClass and DisposalClass'\n",
    "# ----------------------------------------------------------------------------------------------------------------\n",
    "\n",
    "TunnExcvDF[\"BoreClass\"]= np.nan\n",
    "TunnExcvDF[\"SupportClass\"]= np.nan\n",
    "TunnExcvDF[\"DisposalClass\"]= np.nan\n",
    "\n",
    "# instantiate an instance of BoreClass\n",
    "bore_class=BoreClass()\n",
    "# call bore_class methods for BC1, BC2, BC3\n",
    "bore_class.bc1()\n",
    "bore_class.bc2()\n",
    "bore_class.bc3()\n",
    "print TunnExcvDF[\"BoreClass\"].value_counts()  # equals 805+188+60 for Ostroehre\n",
    "print TunnExcvDF[\"ExcavationType\"].value_counts() \n",
    "\n",
    "# Support Class                                                             # JK ToDo: define SC's as Class\n",
    "#  SCT\n",
    "TunnExcvDF.loc[(TunnExcvDF[\"ExcavationType\"] == \"TBM\"), \\\n",
    "    \"SupportClass\"] = \"SCT\"\n",
    "#  SC5\n",
    "TunnExcvDF.loc[(TunnExcvDF[\"ExcavationType\"] == \"MUL\"), \\\n",
    "    \"SupportClass\"] = \"SC5\"\n",
    "# check: TunnExcvDF[\"SupportClass\"].value_counts()\n",
    "# check: TunnExcvDF[\"ExcavationType\"].value_counts() \n",
    "\n",
    "# Disposal Class                                                            # JK ToDo: define MC's as Class\n",
    "#  MC5\n",
    "TunnExcvDF.loc[(TunnExcvDF[\"BoreClass\"] ==  \"BC1\") | (TunnExcvDF[\"BoreClass\"] == \"BC2\"), \\\n",
    "    \"DisposalClass\"] = \"MC5\"\n",
    "#  MC3\n",
    "TunnExcvDF.loc[(TunnExcvDF[\"BoreClass\"] ==  \"BC3\"), \\\n",
    "    \"DisposalClass\"] = \"MC3\"\n",
    "#  MC2\n",
    "TunnExcvDF.loc[(TunnExcvDF[\"ExcavationType\"] == \"MUL\"), \\\n",
    "    \"DisposalClass\"] = \"MC2\"\n",
    "# check: TunnExcvDF[\"DisposalClass\"].value_counts()\n",
    "# check: TunnExcvDF[\"ExcavationType\"].value_counts() # 805+248\n",
    "# check:\n",
    "#     print TunnExcvDF.loc[:,[\"Station\",\"ExcavationType\",\"BoreClass\",\"SupportClass\",\"DisposalClass\"]].to_string()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "calcuating excavation volume\n"
     ]
    }
   ],
   "source": [
    "# ----------------------------------------------------------------------------------------------------------------\n",
    "# calculate excavation volume of tunnel between two axis points\n",
    "print 'calcuating excavation volume'\n",
    "# ----------------------------------------------------------------------------------------------------------------\n",
    "\n",
    "# initialize interval length (StationInterval field)\n",
    "TunnExcvDF[\"StationInterval\"] = np.nan\n",
    "TunnExcvDF[\"ExcavationVolume\"] = np.nan\n",
    "\n",
    "# Calculate \"StationInterval\", \"Area1_mean_dist\" and \"Area2_mean_dist\"\n",
    "n = 0\n",
    "\n",
    "# use .iat instead of .iloc to return scalar values (*1000 faster)\n",
    "# LayoutData must show missing data as NaN (None is read as string value)\n",
    "for i in range(len(TunnExcvDF.index) -1):\n",
    "    nn= n+1\n",
    "    TunnExcvDF[\"StationInterval\"].iat[n] = ((TunnExcvDF[\"Easting\"].iat[nn] -TunnExcvDF[\"Easting\"].iat[n])**2 \n",
    "        +(TunnExcvDF[\"Northing\"].iat[nn] -TunnExcvDF[\"Northing\"].iat[n])**2 \n",
    "        +(TunnExcvDF[\"Elevation\"].iat[nn] -TunnExcvDF[\"Elevation\"].iat[n])**2 )**(0.5)\n",
    "    TunnExcvDF[\"ExcavationVolume\"].iat[n] = TunnExcvDF[\"SectionArea\"].iat[n] * TunnExcvDF[\"StationInterval\"].iat[n]\n",
    "    n = n+1\n",
    "# check:\n",
    "#    print TunnExcvDF.loc[:,[\"Station\",\"ExcavationType\",\"StationInterval\",\"ExcavationVolume\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "calcuating disposal volume\n"
     ]
    }
   ],
   "source": [
    "# ----------------------------------------------------------------------------------------------------------------\n",
    "# calculate disposal volume of tunnel between two axis points\n",
    "print 'calcuating disposal volume'\n",
    "# result: file TunnelExcavationData as .csv)\n",
    "# ----------------------------------------------------------------------------------------------------------------\n",
    "\n",
    "# mv to beginning of file                                                        JK ToDo\n",
    "def disposal_volume(ExcavationVolume, DisposalClass):\n",
    "    #calculate Disposal Volumes based on Disposal Class\n",
    "    DisposalVolume=np.nan\n",
    "    if DisposalClass==\"MC2\":\n",
    "        DisposalVolume=1.3*ExcavationVolume\n",
    "    elif DisposalClass==\"MC3\":\n",
    "        DisposalVolume=1.5*ExcavationVolume \n",
    "    elif DisposalClass==\"MC5\":\n",
    "        DisposalVolume=1.3*ExcavationVolume\n",
    "    #else:\n",
    "        #print \"unknown disposal class\"\n",
    "    return DisposalVolume\n",
    "\n",
    "TunnExcvDF[\"DisposalVolume\"] = np.nan\n",
    "n = 0\n",
    "for i in range(len(TunnExcvDF.index) -1):\n",
    "    TunnExcvDF[\"DisposalVolume\"].iat[n] = (\n",
    "        disposal_volume(TunnExcvDF[\"ExcavationVolume\"].iat[n],TunnExcvDF[\"DisposalClass\"].iat[n]) )\n",
    "    n = n+1\n",
    "# check:\n",
    "#  print TunnExcvDF.loc[:,[\"Station\",\"DisposalType\",\"ExcavationVolume\",\"DisposalVolume\"]]\n",
    "\n",
    "TunnExcvDF.to_csv(TunnelExcavationData, sep=\",\", na_rep=\"NaN\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "creating BoQ\n",
      "111a UEX MUL 205+490 205+520 SC5 3604.43365391 m3\n",
      "111a UEX MUL 205+490 205+520 MC2 4685.76375008 m3\n",
      "111b UEX TBM 205+520 208+620 BC1 23243.5981961 m3\n",
      "111b UEX TBM 205+520 208+620 BC2 72922.244378 m3\n",
      "111b UEX TBM 205+520 208+620 BC3 315378.985687 m3\n",
      "111b UEX TBM 205+520 208+620 SCT 411544.828261 m3\n",
      "111b UEX TBM 205+520 208+620 MC5 125015.595346 m3\n",
      "111b UEX TBM 205+520 208+620 MC3 473068.478531 m3\n",
      "111c UEX MUL 208+620 208+740 SC5 14405.4459974 m3\n",
      "111c UEX MUL 208+620 208+740 MC2 18727.0797966 m3\n"
     ]
    }
   ],
   "source": [
    "# ----------------------------------------------------------------------------------------------------------------\n",
    "# create BoQ and write to file\n",
    "# results: BoQ_df and BoQ as .csv\n",
    "print 'creating BoQ'\n",
    "# ----------------------------------------------------------------------------------------------------------------\n",
    "# replace print with write to file                                             ToDo JK\n",
    "\n",
    "# initialize a BoQ_list\n",
    "BoQ_list_headers= [\"WBS\",\"WorkType\",\"ExcavationType\",\"StationFrom\",\"StationTo\",\"PayItem\",\"Quantity\",\"Unit\"]\n",
    "BoQ_list_values=[]\n",
    "\n",
    "# find combinations of WBScode, ExcavationType and [BoreClass | Support Class | Disposal Class that exist\n",
    "# calculate excavation volume for each combination\n",
    "for i in TunnExcvDF[\"WBScode\"].unique():\n",
    "    for j in TunnExcvDF[\"ExcavationType\"].unique():\n",
    "        if ((TunnExcvDF[\"WBScode\"] == i)\n",
    "             & (TunnExcvDF[\"ExcavationType\"] == j)).any():\n",
    "            work_type = (TunnExcvDF.loc[\n",
    "                ((TunnExcvDF[\"WBScode\"] == i)\n",
    "                & (TunnExcvDF[\"ExcavationType\"] == j)),\"WorkType\"]).unique()[0]\n",
    "        for k in TunnExcvDF[\"BoreClass\"].unique():\n",
    "            # if DF record with i, j, k (as Bore Class) exists:\n",
    "            if ((TunnExcvDF[\"WBScode\"] == i)\n",
    "                & (TunnExcvDF[\"ExcavationType\"] == j)\n",
    "                & (TunnExcvDF[\"BoreClass\"] == k)).any():\n",
    "                start_station = min(TunnExcvDF.loc[\n",
    "                    ((TunnExcvDF[\"WBScode\"] == i)\n",
    "                     & (TunnExcvDF[\"ExcavationType\"] == j)),\"Station\"])\n",
    "                end_station = max(TunnExcvDF.loc[\n",
    "                    ((TunnExcvDF[\"WBScode\"] == i)\n",
    "                     & (TunnExcvDF[\"ExcavationType\"] == j)),\"Station\"])\n",
    "                #need 'Station +1' because we are going From: To: along alignment\n",
    "                #TunnExcvDF.loc[(TunnExcvDF[\"Station\"] == end_station),\"Station\"].values[0]    ToDo Note JK\n",
    "                end_station_index=TunnExcvDF.index.get_loc(\n",
    "                    TunnExcvDF.loc[(TunnExcvDF[\"Station\"] == end_station),\"Station\"].index[0]) +1\n",
    "                end_station=TunnExcvDF.iloc[end_station_index,TunnExcvDF.columns.get_loc(\"Station\")]\n",
    "                volume_sum=TunnExcvDF.loc[\n",
    "                    ((TunnExcvDF[\"WBScode\"] == i)\n",
    "                     & (TunnExcvDF[\"ExcavationType\"] == j)\n",
    "                     & (TunnExcvDF[\"BoreClass\"] == k)),\"ExcavationVolume\"].sum()\n",
    "                BoQ_list_values.append((i,work_type,j,start_station,end_station,k,volume_sum,volume_unit))\n",
    "                print i, work_type, j, start_station, end_station, k, volume_sum, volume_unit\n",
    "        for k in TunnExcvDF[\"SupportClass\"].unique():\n",
    "            # if DF record with i, j, k (as Support Class) exists:\n",
    "            if ((TunnExcvDF[\"WBScode\"] == i)\n",
    "                & (TunnExcvDF[\"ExcavationType\"] == j)\n",
    "                & (TunnExcvDF[\"SupportClass\"] == k)).any():                    \n",
    "                start_station = min(TunnExcvDF.loc[\n",
    "                    ((TunnExcvDF[\"WBScode\"] == i)\n",
    "                     & (TunnExcvDF[\"ExcavationType\"] == j)),\"Station\"])\n",
    "                end_station = max(TunnExcvDF.loc[\n",
    "                    ((TunnExcvDF[\"WBScode\"] == i)\n",
    "                     & (TunnExcvDF[\"ExcavationType\"] == j)),\"Station\"])\n",
    "                end_station_index=TunnExcvDF.index.get_loc(\n",
    "                    TunnExcvDF.loc[(TunnExcvDF[\"Station\"] == end_station),\"Station\"].index[0]) +1\n",
    "                end_station=TunnExcvDF.iloc[end_station_index,TunnExcvDF.columns.get_loc(\"Station\")]\n",
    "                volume_sum=TunnExcvDF.loc[\n",
    "                    ((TunnExcvDF[\"WBScode\"] == i)\n",
    "                     & (TunnExcvDF[\"ExcavationType\"] == j)\n",
    "                     & (TunnExcvDF[\"SupportClass\"] == k)),\"ExcavationVolume\"].sum()\n",
    "                BoQ_list_values.append((i,work_type,j,start_station,end_station,k,volume_sum,volume_unit))\n",
    "                print i, work_type, j, start_station, end_station, k, volume_sum, volume_unit\n",
    "        for k in TunnExcvDF[\"DisposalClass\"].unique():\n",
    "            # if DF record with i, j, k (as Support Class) exists:\n",
    "            if ((TunnExcvDF[\"WBScode\"] == i)\n",
    "                & (TunnExcvDF[\"ExcavationType\"] == j)\n",
    "                & (TunnExcvDF[\"DisposalClass\"] == k)).any():                    \n",
    "                start_station = min(TunnExcvDF.loc[\n",
    "                    ((TunnExcvDF[\"WBScode\"] == i)\n",
    "                     & (TunnExcvDF[\"ExcavationType\"] == j)),\"Station\"])\n",
    "                end_station = max(TunnExcvDF.loc[\n",
    "                    ((TunnExcvDF[\"WBScode\"] == i)\n",
    "                     & (TunnExcvDF[\"ExcavationType\"] == j)),\"Station\"])\n",
    "                end_station_index=TunnExcvDF.index.get_loc(\n",
    "                    TunnExcvDF.loc[(TunnExcvDF[\"Station\"] == end_station),\"Station\"].index[0]) +1\n",
    "                end_station=TunnExcvDF.iloc[end_station_index,TunnExcvDF.columns.get_loc(\"Station\")]\n",
    "                volume_sum=TunnExcvDF.loc[\n",
    "                    ((TunnExcvDF[\"WBScode\"] == i)\n",
    "                     & (TunnExcvDF[\"ExcavationType\"] == j)\n",
    "                     & (TunnExcvDF[\"DisposalClass\"] == k)),\"DisposalVolume\"].sum()\n",
    "                BoQ_list_values.append((i,work_type,j,start_station,end_station,k,volume_sum,volume_unit))\n",
    "                print i, work_type, j, start_station, end_station, k, volume_sum, volume_unit\n",
    "# check:\n",
    "#print TunnExcvDF.loc[TunnExcvDF[\"ExcavationType\"] == \"TBM\", \"ExcavationVolume\"].sum()\n",
    "#print TunnExcvDF.loc[TunnExcvDF[\"ExcavationType\"] == \"TBM\", \"DisposalVolume\"].sum()\n",
    "\n",
    "BoQ_df =  pd.DataFrame(BoQ_list_values, columns=BoQ_list_headers).round(decimals=3)\n",
    "BoQ_df.to_csv(BoQ, sep=\",\", na_rep=\"NaN\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
