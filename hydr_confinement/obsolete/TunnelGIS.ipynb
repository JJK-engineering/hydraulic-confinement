{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# python setup for qgis processing\n",
    "\n",
    "import sys\n",
    "from qgis.core import *\n",
    "# need qgis,gui ?\n",
    "#from qgis.gui import *\n",
    "# need PyQt4.QtCore ?\n",
    "#from PyQt4.QtCore import *\n",
    "from PyQt4.QtGui import *\n",
    "\n",
    "# what does True refer to below ?\n",
    "app = QApplication([], True)\n",
    "QgsApplication.setPrefixPath(\"/usr\", True)\n",
    "# /usr correct?\n",
    "#QgsApplication.setPrefixPath(qgis_path, True)\n",
    "QgsApplication.initQgis()\n",
    "\n",
    "sys.path.append('/usr/share/qgis/python/plugins')\n",
    "from processing.core.Processing import Processing\n",
    "Processing.initialize()\n",
    "from processing.tools import *\n",
    "\n",
    "#why is this still needed\n",
    "import processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# ----------------------------------------------------------------------------------------------------------------\n",
    "# description\n",
    "# ----------------------------------------------------------------------------------------------------------------\n",
    "\n",
    "#!/usr/bin/python\n",
    "# TunnelExcavationData.py\n",
    "\n",
    "# Python procedure for TunnelGIS Engineering App\n",
    "# Author: KK\n",
    "# Date: 01.04.2017\n",
    "\n",
    "# Purpose of this procedure:\n",
    "# 1. Prepare input data ....\n",
    "# 2.\n",
    "# 3.\n",
    "# 4. \n",
    "# 5. \n",
    "\n",
    "# This python routine is a script, intended to guide the user through the described procedure.\n",
    "# As a script, the procedure does not generally include data validation and error handling.\n",
    "# Users are expected to understand and adjust the code as needed for their application.\n",
    "\n",
    "# Required Input Files:\n",
    "#   \"WORK/swissalti3dgeotifflv03-5m/swissALTI3D_.tif\"   -DEM with surface topography\n",
    "#   \"WORK/Felsisohypsen-raster.tif\"                     -DEM with rock surface\n",
    "#   \"WORK/OstrohrR2.csv\"                                -stationed tunnel alignment#\n",
    "#   \"WORK/Ostroehre.TunnelLayoutData.R2.csv\"            -tunnel layout data\n",
    "\n",
    "# References:\n",
    "# http://gis.stackexchange.com/questions/197825/how-to-convert-multiple-csv-files-to-shp-using-python-and-no-arcpy\n",
    "# to get grass help:   processing.alghelp(\"grass7:r.what.points\")\n",
    "\n",
    "# IMPORTANT: requires qgis setup before running this procedure\n",
    "# run ./pyqgis.sh from command line before starting python (or set up IDE accordingly)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# ----------------------------------------------------------------------------------------------------------------\n",
    "# import required libraries\n",
    "# ----------------------------------------------------------------------------------------------------------------\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import geopandas as gpd\n",
    "import shapely as sp\n",
    "import os "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# ----------------------------------------------------------------------------------------------------------------\n",
    "# set wd for this procedure \n",
    "# ----------------------------------------------------------------------------------------------------------------\n",
    "os.chdir(\"/home/kaelin_joseph/TunnelGIS.Rheintunnel/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# ----------------------------------------------------------------------------------------------------------------\n",
    "# define input files\n",
    "# ----------------------------------------------------------------------------------------------------------------\n",
    "\n",
    "DTM = \"WORK/swissalti3dgeotifflv03-5m/swissALTI3D_.tif\"  \n",
    "RockSurface = \"WORK/Felsisohypsen-raster.tif\"            \n",
    "AlignmentData = \"WORK/Ostroehre.AlignmentData.R2.csv\"\n",
    "LayoutData = \"WORK/Ostroehre.TunnelLayoutData.R2.csv\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# ----------------------------------------------------------------------------------------------------------------\n",
    "# define Bore Classes\n",
    "#   define Bore Classes as class, to separate definition of methods from execution\n",
    "#   class method is used as a modifier to the TunnelExcavationData (dataframe) class.\n",
    "# ----------------------------------------------------------------------------------------------------------------\n",
    "# better to add mtethods .bc1, .bc2, .bc3 to TunnExcvDf ??\n",
    "\n",
    "tunn_h =13.0   # define tunnel height\n",
    "volume_unit='m3'  # unit to be used for volume calculation and reporting\n",
    "\n",
    "class BoreClass:\n",
    "    \"\"\"Determine Bore Class for TBM tunnels\"\"\"\n",
    "    # BC1 - tunnel predominantly in soil\n",
    "    def bc1(self):\n",
    "        TunnExcvDF.loc[(TunnExcvDF[\"ExcavationType\"] == \"TBM\") & \n",
    "        (TunnExcvDF[\"RockSurface\"] <= TunnExcvDF[\"Elevation\"] -tunn_h*0.25),\"BoreClass\"] \\\n",
    "        =\"BC1\"\n",
    "    # BC2 - tunnel with mixed face\n",
    "    def bc2(self):\n",
    "        TunnExcvDF.loc[(TunnExcvDF[\"ExcavationType\"] == \"TBM\") & \n",
    "        (TunnExcvDF[\"RockSurface\"] > TunnExcvDF[\"Elevation\"] -tunn_h*0.25) & \n",
    "        (TunnExcvDF[\"RockSurface\"] < TunnExcvDF[\"Elevation\"] +tunn_h/2.0 +1.5),\"BoreClass\"] \\\n",
    "        = \"BC2\"\n",
    "    # BC3 - tunnel inf rock\n",
    "    def bc3(self):\n",
    "        TunnExcvDF.loc[(TunnExcvDF[\"ExcavationType\"] == \"TBM\") & \\\n",
    "        (TunnExcvDF[\"RockSurface\"] >= TunnExcvDF[\"Elevation\"] +tunn_h/2.0 +1.5),\"BoreClass\"] \\\n",
    "        = \"BC3\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# ----------------------------------------------------------------------------------------------------------------\n",
    "# define output files\n",
    "# ----------------------------------------------------------------------------------------------------------------\n",
    "\n",
    "TunnelExcavationData = \"WORK/Ostroehre.TunnelExcavationData.R2.csv\"\n",
    "# headers: Station, Easting, Northing, Elevation, DTM, RockSurface, StationReal, RockCover,\n",
    "#          WBScode, WorkType, ExcavationType, ProfileType, SectionArea, Description,\n",
    "#          BoreClass, SupportClass, DisposalClass, StationInterval, ExcavationVolume, DisposalVolume\n",
    "Alignment_SHP ='WORK/Ostroehre.Alignment.R2.shp'\n",
    "BoQ = \"WORK/Ostroehre.TunnelBoQdata.R2.csv\"\n",
    "# temporary data\n",
    "Alignment_DTM = \"WORK/Ostroehre.Terrain.R2.csv\"\n",
    "Alignment_RockSurface = \"WORK/Ostroehre.RockSurface.R2.csv\"  # JK ToDo: RockSurface?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# ----------------------------------------------------------------------------------------------------------------\n",
    "# create alignment_df (dataframe) from .csv\n",
    "# Important: Before the df is created the data should be checked.\n",
    "#   E.g. make sure that it does not contain trailing blank lines and that duplicate lines are deleted.\n",
    "# result: alignment_df\n",
    "# ----------------------------------------------------------------------------------------------------------------\n",
    "\n",
    "alignment_df = pd.read_csv(AlignmentData)\n",
    "#delete row if only NA are present in row\n",
    "alignment_df = alignment_df.dropna(how = \"all\")\n",
    "# round alignment_df to three decimals\n",
    "alignment_df = alignment_df.round(decimals=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#alignment_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# ----------------------------------------------------------------------------------------------------------------\n",
    "# create layout_df from .csv\n",
    "# result: layout_df\n",
    "# ----------------------------------------------------------------------------------------------------------------\n",
    "\n",
    "layout_df = pd.read_csv(LayoutData)\n",
    "# round layout_df to three decimals\n",
    "layout_df = layout_df.round(decimals=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#layout_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "creating df header StationReal\n"
     ]
    }
   ],
   "source": [
    "# ----------------------------------------------------------------------------------------------------------------\n",
    "# convert alignment_df[\"Station\"] => alignment_df[\"StationReal\"] and similar for layout_df\n",
    "# result: alignment_Station_list, layout_Station_list\n",
    "#         alignment_df[\"StationReal\"], layout_df[\"StationReal\"]\n",
    "print \"creating df header StationReal\"\n",
    "# ----------------------------------------------------------------------------------------------------------------\n",
    "\n",
    "alignment_Station_list = alignment_df[\"Station\"].tolist()\n",
    "    # check: len(alignment_Station_list)\n",
    "\n",
    "alignment_df[\"StationReal\"] = np.nan\n",
    "\n",
    "for n in range(0, len(alignment_Station_list)):\n",
    "    station_sel = alignment_df.iloc[n][\"Station\"]\n",
    "    station_real_sel = float(station_sel.replace(\"+\",\"\"))\n",
    "    alignment_df.iloc[n, alignment_df.columns.get_loc(\"StationReal\")] = station_real_sel\n",
    "    # alignment_df.columns.get_loc(\"StationReal\") = 5\n",
    "\n",
    "# layout_df[\"Station\"] => layout_df[\"StationReal\"] \n",
    "layout_Station_list = layout_df[\"Station\"].tolist()\n",
    "    # check: len(layout_Station_list)\n",
    "\n",
    "layout_df[\"StationReal\"] = np.nan\n",
    "\n",
    "for n in range(0, len(layout_Station_list)):\n",
    "    station_sel = layout_df.iloc[n][\"Station\"]\n",
    "    station_real_sel = float(station_sel.replace(\"+\",\"\"))\n",
    "    layout_df.iloc[n, layout_df.columns.get_loc(\"StationReal\")] \\\n",
    "        = station_real_sel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "adding Stations\n",
      "            Point Type  Station     Northing      Easting  Elevation  StationReal\n",
      "427  205+488  NaN  205+488  1268944.416  2612475.076    248.149     205488.0\n",
      "            Point Type  Station     Northing      Easting  Elevation  StationReal\n",
      "429  205+491  NaN  205+491  1268941.813  2612476.566    247.999     205491.0\n",
      "    station_real 205490.0\n",
      "    easting_newpoint_sel 2612476.06957\n",
      "    northing_newpoint_sel 1268942.68025\n",
      "    elevation_newpoint_sel 248.048976153\n",
      "            Point Type  Station     Northing      Easting  Elevation  StationReal\n",
      "447  205+518  NaN  205+518  1268918.379  2612489.977    246.648     205518.0\n",
      "            Point Type  Station     Northing      Easting  Elevation  StationReal\n",
      "449  205+521  NaN  205+521  1268915.775  2612491.468    246.498     205521.0\n",
      "    station_real 205520.0\n",
      "    easting_newpoint_sel 2612490.97078\n",
      "    northing_newpoint_sel 1268916.64338\n",
      "    elevation_newpoint_sel 246.548021643\n"
     ]
    }
   ],
   "source": [
    "# ----------------------------------------------------------------------------------------------------------------\n",
    "# check if every layout_df[\"StationReal\"] exists in alignment_df[\"StationReal\"]\n",
    "#   If it does not exist, create a new Station in alignment_df\n",
    "# result: alignment_StationReal_list, layout_StationReal_list\n",
    "#         alignment_df with added Stations\n",
    "print \"adding Stations\"\n",
    "# ----------------------------------------------------------------------------------------------------------------\n",
    "\n",
    "alignment_StationReal_list = alignment_df[\"StationReal\"].tolist()\n",
    "layout_StationReal_list = layout_df[\"StationReal\"].tolist()\n",
    "\n",
    "# within this vicinity of a station no new station will be created\n",
    "vicinity = 0.1\n",
    "\n",
    "# define new variables\n",
    "easting_newpoint = []\n",
    "northing_newpoint = []\n",
    "elevation_newpoint = []\n",
    "station = []\n",
    "station_real = []\n",
    "\n",
    "# Loop through stations\n",
    "for n in layout_StationReal_list:\n",
    "    if n in alignment_StationReal_list:\n",
    "        pass\n",
    "    else:\n",
    "        neighbour1_StationReal = max([i for i in alignment_StationReal_list if i < n]) \n",
    "        neighbour2_StationReal = min([i for i in alignment_StationReal_list if i > n])       \n",
    "        ####if n < neighbour1_StationReal + vicinity or n > neighbour2_StationReal + vicinity:      KLK: check\n",
    "        if n < neighbour1_StationReal + vicinity or n > neighbour2_StationReal - vicinity:\n",
    "            pass\n",
    "        else: \n",
    "            neighbour1 = alignment_df.loc[alignment_df[\"StationReal\"]\n",
    "                                          == neighbour1_StationReal,]\n",
    "            neighbour2 = alignment_df.loc[alignment_df[\"StationReal\"]\n",
    "                                          == neighbour2_StationReal,]\n",
    "            ####delta_x_neighbour1_2 = abs(neighbour2.Easting.tolist()[0] -neighbour1.Easting.tolist()[0])\n",
    "            ####delta_y_neighbour1_2 = abs(neighbour2.Northing.tolist()[0] -neighbour1.Northing.tolist()[0])\n",
    "            ####                                                                                  KLK: check\n",
    "            delta_x_neighbour1_2 = neighbour2.Easting.tolist()[0] -neighbour1.Easting.tolist()[0] #delta x\n",
    "            delta_y_neighbour1_2 = neighbour2.Northing.tolist()[0] -neighbour1.Northing.tolist()[0] #delta y\n",
    "            delta_z_neighbour1_2 = neighbour2.Elevation.tolist()[0] -neighbour1.Elevation.tolist()[0] #delta z\n",
    "            length_neighbour1_2 = (delta_x_neighbour1_2**2 +delta_y_neighbour1_2**2)**(0.5) # L\n",
    "            length_neighbour1_newpoint = n- neighbour1.StationReal.tolist()[0]\n",
    "            ####easting_newpoint_sel = neighbour2.Easting.tolist()[0] \\\n",
    "            ####                     +((delta_y_neighbour1_2*length_neighbour1_newpoint)/length_neighbour1_2)\n",
    "            ####northing_newpoint_sel = neighbour2.Northing.tolist()[0] \\\n",
    "            ####                      +((delta_x_neighbour1_2*length_neighbour1_newpoint)/length_neighbour1_2)\n",
    "            ####elevation_newpoint_sel = neighbour2.Elevation.tolist()[0] \\\n",
    "            ####                       +((delta_x_neighbour1_2*length_neighbour1_newpoint)/length_neighbour1_2)\n",
    "            ####                                                                                  KLK: check\n",
    "            easting_newpoint_sel = neighbour1.Easting.tolist()[0] \\\n",
    "                                 +((delta_x_neighbour1_2*length_neighbour1_newpoint)/length_neighbour1_2)\n",
    "            northing_newpoint_sel = neighbour1.Northing.tolist()[0] \\\n",
    "                                  +((delta_y_neighbour1_2*length_neighbour1_newpoint)/length_neighbour1_2)\n",
    "            elevation_newpoint_sel = neighbour1.Elevation.tolist()[0] \\\n",
    "                                   +((delta_z_neighbour1_2*length_neighbour1_newpoint)/length_neighbour1_2)\n",
    "            easting_newpoint.append(easting_newpoint_sel)\n",
    "            northing_newpoint.append(northing_newpoint_sel)\n",
    "            elevation_newpoint.append(elevation_newpoint_sel)\n",
    "            station_real.append(n)\n",
    "            print \"    \", neighbour1\n",
    "            print \"    \", neighbour2\n",
    "            print \"    station_real\", n\n",
    "            print \"    easting_newpoint_sel\", easting_newpoint_sel\n",
    "            print \"    northing_newpoint_sel\", northing_newpoint_sel\n",
    "            print \"    elevation_newpoint_sel\", elevation_newpoint_sel\n",
    "            # this procedure must be tested for all combinations of ascending/descending\n",
    "            #   Northing, Easting and Elevation --> should be OK\n",
    "            #   for descending Stationing --> needs fixing                                          JK ToDo\n",
    "            Station_sel = layout_df.loc[layout_df['StationReal']\n",
    "                                                  == n, 'Station'] \n",
    "            station.append(Station_sel.tolist()[0])\n",
    "           \n",
    "newStation_df = pd.DataFrame({\"Easting\": easting_newpoint, \"Northing\": northing_newpoint,\n",
    "                              \"Elevation\": elevation_newpoint, \"StationReal\": station_real,\n",
    "                              \"Station\": station})\n",
    "    # check len(alignment_df)\n",
    "    # check len(newStation_df)\n",
    "\n",
    "# Contatenate alignment_df with newStation_df\n",
    "# result: alignment_df\n",
    "frames = [alignment_df, newStation_df]\n",
    "alignment_df = pd.concat(frames)\n",
    "    # check: len(alignment_df)\n",
    "    # check: alignment_df.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ----------------------------------------------------------------------------------------------------------------\n",
    "# create Alignment_spatial from alignment_df\n",
    "# result: Alignment_SHP\n",
    "# ----------------------------------------------------------------------------------------------------------------\n",
    "\n",
    "Alignment_spatial_points = [sp.geometry.Point(row['Easting'], row['Northing'])\n",
    "                            for key, row in alignment_df.iterrows()]\n",
    "Alignment_crs = {'init': 'epsg:2056'}  #define crs\n",
    "Alignment_spatial = gpd.GeoDataFrame(alignment_df, geometry=Alignment_spatial_points, crs = Alignment_crs)\n",
    "Alignment_spatial.to_file(Alignment_SHP, driver='ESRI Shapefile') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "get raster values\n",
      "Warning: Not all input layers use the same CRS.\n",
      "This can cause unexpected results.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'output': 'WORK/Ostroehre.RockSurface.R2.csv'}"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# ----------------------------------------------------------------------------------------------------------------\n",
    "# use grass functions to get raster values for points along tunnel axis and write to .csv files\n",
    "# result: Alignment_DTM, Alignment_RockSurface\n",
    "print \"get raster values\"\n",
    "# ----------------------------------------------------------------------------------------------------------------\n",
    "\n",
    "# Alignment_DTM\n",
    "processing.runalg(\"grass7:r.what.points\",DTM,Alignment_SHP,\n",
    "                  \"NA\",\",\",500,True,False,False,False,False,\n",
    "                  \"2603510.0,2624270.0,1260650.0,1274890.0\",-1,0.0001,Alignment_DTM)\n",
    "# Alignment_RockSurface                  \n",
    "processing.runalg(\"grass7:r.what.points\",RockSurface,Alignment_SHP,\n",
    "                  \"NA\",\",\",500, True,False,False,False,False,\n",
    "                  \"2603510.0,2624270.0,1260650.0,1274890.0\",-1,0.0001,Alignment_RockSurface)\n",
    "## warning: Not all input layers use the same CRS -> data seems OK\n",
    "    # check:  Alginemnt_spatial.crs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# ----------------------------------------------------------------------------------------------------------------\n",
    "# create df's\n",
    "# result: Alignment_DTM_df, Alignment_RockSurface_df\n",
    "# ----------------------------------------------------------------------------------------------------------------\n",
    "Alignment_DTM_df  = pd.read_csv(Alignment_DTM)\n",
    "Alignment_RockSurface_df  = pd.read_csv(Alignment_RockSurface)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>easting</th>\n",
       "      <th>northing</th>\n",
       "      <th>site_name</th>\n",
       "      <th>tmp1516293888222</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2.612072e+06</td>\n",
       "      <td>1.269294e+06</td>\n",
       "      <td>NaN</td>\n",
       "      <td>253.5240</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2.612073e+06</td>\n",
       "      <td>1.269295e+06</td>\n",
       "      <td>NaN</td>\n",
       "      <td>253.6122</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2.612075e+06</td>\n",
       "      <td>1.269297e+06</td>\n",
       "      <td>NaN</td>\n",
       "      <td>253.1596</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2.612078e+06</td>\n",
       "      <td>1.269299e+06</td>\n",
       "      <td>NaN</td>\n",
       "      <td>253.1596</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2.612080e+06</td>\n",
       "      <td>1.269301e+06</td>\n",
       "      <td>NaN</td>\n",
       "      <td>252.9344</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>2.612082e+06</td>\n",
       "      <td>1.269303e+06</td>\n",
       "      <td>NaN</td>\n",
       "      <td>252.9344</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>2.612085e+06</td>\n",
       "      <td>1.269304e+06</td>\n",
       "      <td>NaN</td>\n",
       "      <td>252.9344</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>2.612087e+06</td>\n",
       "      <td>1.269306e+06</td>\n",
       "      <td>NaN</td>\n",
       "      <td>252.5740</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>2.612090e+06</td>\n",
       "      <td>1.269308e+06</td>\n",
       "      <td>NaN</td>\n",
       "      <td>252.0129</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>2.612093e+06</td>\n",
       "      <td>1.269309e+06</td>\n",
       "      <td>NaN</td>\n",
       "      <td>252.0129</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>2.612095e+06</td>\n",
       "      <td>1.269311e+06</td>\n",
       "      <td>NaN</td>\n",
       "      <td>251.8054</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>2.612098e+06</td>\n",
       "      <td>1.269312e+06</td>\n",
       "      <td>NaN</td>\n",
       "      <td>251.8054</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>2.612101e+06</td>\n",
       "      <td>1.269313e+06</td>\n",
       "      <td>NaN</td>\n",
       "      <td>251.6312</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>2.612103e+06</td>\n",
       "      <td>1.269315e+06</td>\n",
       "      <td>NaN</td>\n",
       "      <td>251.6312</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>2.612106e+06</td>\n",
       "      <td>1.269316e+06</td>\n",
       "      <td>NaN</td>\n",
       "      <td>251.1492</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>2.612109e+06</td>\n",
       "      <td>1.269317e+06</td>\n",
       "      <td>NaN</td>\n",
       "      <td>251.1492</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>2.612112e+06</td>\n",
       "      <td>1.269318e+06</td>\n",
       "      <td>NaN</td>\n",
       "      <td>251.1567</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>2.612114e+06</td>\n",
       "      <td>1.269319e+06</td>\n",
       "      <td>NaN</td>\n",
       "      <td>251.1567</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>2.612117e+06</td>\n",
       "      <td>1.269320e+06</td>\n",
       "      <td>NaN</td>\n",
       "      <td>250.6956</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>2.612118e+06</td>\n",
       "      <td>1.269320e+06</td>\n",
       "      <td>NaN</td>\n",
       "      <td>250.6956</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>2.612120e+06</td>\n",
       "      <td>1.269321e+06</td>\n",
       "      <td>NaN</td>\n",
       "      <td>250.6664</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>2.612123e+06</td>\n",
       "      <td>1.269322e+06</td>\n",
       "      <td>NaN</td>\n",
       "      <td>250.6664</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>2.612126e+06</td>\n",
       "      <td>1.269323e+06</td>\n",
       "      <td>NaN</td>\n",
       "      <td>250.6372</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>2.612129e+06</td>\n",
       "      <td>1.269324e+06</td>\n",
       "      <td>NaN</td>\n",
       "      <td>250.6372</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>2.612132e+06</td>\n",
       "      <td>1.269324e+06</td>\n",
       "      <td>NaN</td>\n",
       "      <td>250.6380</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>2.612135e+06</td>\n",
       "      <td>1.269325e+06</td>\n",
       "      <td>NaN</td>\n",
       "      <td>250.6380</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>2.612137e+06</td>\n",
       "      <td>1.269325e+06</td>\n",
       "      <td>NaN</td>\n",
       "      <td>248.9911</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>2.612140e+06</td>\n",
       "      <td>1.269326e+06</td>\n",
       "      <td>NaN</td>\n",
       "      <td>249.5887</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>2.612143e+06</td>\n",
       "      <td>1.269326e+06</td>\n",
       "      <td>NaN</td>\n",
       "      <td>249.5887</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>2.612146e+06</td>\n",
       "      <td>1.269327e+06</td>\n",
       "      <td>NaN</td>\n",
       "      <td>250.4562</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1563</th>\n",
       "      <td>2.614585e+06</td>\n",
       "      <td>1.266036e+06</td>\n",
       "      <td>NaN</td>\n",
       "      <td>270.0327</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1564</th>\n",
       "      <td>2.614584e+06</td>\n",
       "      <td>1.266034e+06</td>\n",
       "      <td>NaN</td>\n",
       "      <td>269.9010</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1565</th>\n",
       "      <td>2.614583e+06</td>\n",
       "      <td>1.266031e+06</td>\n",
       "      <td>NaN</td>\n",
       "      <td>269.9010</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1566</th>\n",
       "      <td>2.614582e+06</td>\n",
       "      <td>1.266028e+06</td>\n",
       "      <td>NaN</td>\n",
       "      <td>270.0154</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1567</th>\n",
       "      <td>2.614581e+06</td>\n",
       "      <td>1.266025e+06</td>\n",
       "      <td>NaN</td>\n",
       "      <td>270.0154</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1568</th>\n",
       "      <td>2.614580e+06</td>\n",
       "      <td>1.266022e+06</td>\n",
       "      <td>NaN</td>\n",
       "      <td>269.9168</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1569</th>\n",
       "      <td>2.614579e+06</td>\n",
       "      <td>1.266019e+06</td>\n",
       "      <td>NaN</td>\n",
       "      <td>269.7921</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1570</th>\n",
       "      <td>2.614578e+06</td>\n",
       "      <td>1.266017e+06</td>\n",
       "      <td>NaN</td>\n",
       "      <td>269.7921</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1571</th>\n",
       "      <td>2.614577e+06</td>\n",
       "      <td>1.266014e+06</td>\n",
       "      <td>NaN</td>\n",
       "      <td>269.8804</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1572</th>\n",
       "      <td>2.614576e+06</td>\n",
       "      <td>1.266011e+06</td>\n",
       "      <td>NaN</td>\n",
       "      <td>269.8804</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1573</th>\n",
       "      <td>2.614575e+06</td>\n",
       "      <td>1.266008e+06</td>\n",
       "      <td>NaN</td>\n",
       "      <td>269.9128</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1574</th>\n",
       "      <td>2.614574e+06</td>\n",
       "      <td>1.266005e+06</td>\n",
       "      <td>NaN</td>\n",
       "      <td>269.7603</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1575</th>\n",
       "      <td>2.614573e+06</td>\n",
       "      <td>1.266002e+06</td>\n",
       "      <td>NaN</td>\n",
       "      <td>269.7228</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1576</th>\n",
       "      <td>2.614572e+06</td>\n",
       "      <td>1.266000e+06</td>\n",
       "      <td>NaN</td>\n",
       "      <td>269.7057</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1577</th>\n",
       "      <td>2.614571e+06</td>\n",
       "      <td>1.265997e+06</td>\n",
       "      <td>NaN</td>\n",
       "      <td>269.7057</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1578</th>\n",
       "      <td>2.614570e+06</td>\n",
       "      <td>1.265994e+06</td>\n",
       "      <td>NaN</td>\n",
       "      <td>269.7682</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1579</th>\n",
       "      <td>2.614569e+06</td>\n",
       "      <td>1.265991e+06</td>\n",
       "      <td>NaN</td>\n",
       "      <td>269.6732</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1580</th>\n",
       "      <td>2.614568e+06</td>\n",
       "      <td>1.265988e+06</td>\n",
       "      <td>NaN</td>\n",
       "      <td>269.6079</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1581</th>\n",
       "      <td>2.614567e+06</td>\n",
       "      <td>1.265985e+06</td>\n",
       "      <td>NaN</td>\n",
       "      <td>269.6079</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1582</th>\n",
       "      <td>2.614566e+06</td>\n",
       "      <td>1.265983e+06</td>\n",
       "      <td>NaN</td>\n",
       "      <td>269.6148</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1583</th>\n",
       "      <td>2.614565e+06</td>\n",
       "      <td>1.265980e+06</td>\n",
       "      <td>NaN</td>\n",
       "      <td>269.6337</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1584</th>\n",
       "      <td>2.614564e+06</td>\n",
       "      <td>1.265977e+06</td>\n",
       "      <td>NaN</td>\n",
       "      <td>269.5825</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1585</th>\n",
       "      <td>2.614563e+06</td>\n",
       "      <td>1.265974e+06</td>\n",
       "      <td>NaN</td>\n",
       "      <td>269.5660</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1586</th>\n",
       "      <td>2.614562e+06</td>\n",
       "      <td>1.265971e+06</td>\n",
       "      <td>NaN</td>\n",
       "      <td>269.5660</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1587</th>\n",
       "      <td>2.614561e+06</td>\n",
       "      <td>1.265968e+06</td>\n",
       "      <td>NaN</td>\n",
       "      <td>269.5902</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1588</th>\n",
       "      <td>2.614560e+06</td>\n",
       "      <td>1.265966e+06</td>\n",
       "      <td>NaN</td>\n",
       "      <td>269.5902</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1589</th>\n",
       "      <td>2.614559e+06</td>\n",
       "      <td>1.265963e+06</td>\n",
       "      <td>NaN</td>\n",
       "      <td>269.3810</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1590</th>\n",
       "      <td>2.614559e+06</td>\n",
       "      <td>1.265961e+06</td>\n",
       "      <td>NaN</td>\n",
       "      <td>269.3810</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1591</th>\n",
       "      <td>2.612476e+06</td>\n",
       "      <td>1.268943e+06</td>\n",
       "      <td>NaN</td>\n",
       "      <td>262.9595</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1592</th>\n",
       "      <td>2.612491e+06</td>\n",
       "      <td>1.268917e+06</td>\n",
       "      <td>NaN</td>\n",
       "      <td>262.8025</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1593 rows Ã— 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "           easting      northing  site_name  tmp1516293888222\n",
       "0     2.612072e+06  1.269294e+06        NaN          253.5240\n",
       "1     2.612073e+06  1.269295e+06        NaN          253.6122\n",
       "2     2.612075e+06  1.269297e+06        NaN          253.1596\n",
       "3     2.612078e+06  1.269299e+06        NaN          253.1596\n",
       "4     2.612080e+06  1.269301e+06        NaN          252.9344\n",
       "5     2.612082e+06  1.269303e+06        NaN          252.9344\n",
       "6     2.612085e+06  1.269304e+06        NaN          252.9344\n",
       "7     2.612087e+06  1.269306e+06        NaN          252.5740\n",
       "8     2.612090e+06  1.269308e+06        NaN          252.0129\n",
       "9     2.612093e+06  1.269309e+06        NaN          252.0129\n",
       "10    2.612095e+06  1.269311e+06        NaN          251.8054\n",
       "11    2.612098e+06  1.269312e+06        NaN          251.8054\n",
       "12    2.612101e+06  1.269313e+06        NaN          251.6312\n",
       "13    2.612103e+06  1.269315e+06        NaN          251.6312\n",
       "14    2.612106e+06  1.269316e+06        NaN          251.1492\n",
       "15    2.612109e+06  1.269317e+06        NaN          251.1492\n",
       "16    2.612112e+06  1.269318e+06        NaN          251.1567\n",
       "17    2.612114e+06  1.269319e+06        NaN          251.1567\n",
       "18    2.612117e+06  1.269320e+06        NaN          250.6956\n",
       "19    2.612118e+06  1.269320e+06        NaN          250.6956\n",
       "20    2.612120e+06  1.269321e+06        NaN          250.6664\n",
       "21    2.612123e+06  1.269322e+06        NaN          250.6664\n",
       "22    2.612126e+06  1.269323e+06        NaN          250.6372\n",
       "23    2.612129e+06  1.269324e+06        NaN          250.6372\n",
       "24    2.612132e+06  1.269324e+06        NaN          250.6380\n",
       "25    2.612135e+06  1.269325e+06        NaN          250.6380\n",
       "26    2.612137e+06  1.269325e+06        NaN          248.9911\n",
       "27    2.612140e+06  1.269326e+06        NaN          249.5887\n",
       "28    2.612143e+06  1.269326e+06        NaN          249.5887\n",
       "29    2.612146e+06  1.269327e+06        NaN          250.4562\n",
       "...            ...           ...        ...               ...\n",
       "1563  2.614585e+06  1.266036e+06        NaN          270.0327\n",
       "1564  2.614584e+06  1.266034e+06        NaN          269.9010\n",
       "1565  2.614583e+06  1.266031e+06        NaN          269.9010\n",
       "1566  2.614582e+06  1.266028e+06        NaN          270.0154\n",
       "1567  2.614581e+06  1.266025e+06        NaN          270.0154\n",
       "1568  2.614580e+06  1.266022e+06        NaN          269.9168\n",
       "1569  2.614579e+06  1.266019e+06        NaN          269.7921\n",
       "1570  2.614578e+06  1.266017e+06        NaN          269.7921\n",
       "1571  2.614577e+06  1.266014e+06        NaN          269.8804\n",
       "1572  2.614576e+06  1.266011e+06        NaN          269.8804\n",
       "1573  2.614575e+06  1.266008e+06        NaN          269.9128\n",
       "1574  2.614574e+06  1.266005e+06        NaN          269.7603\n",
       "1575  2.614573e+06  1.266002e+06        NaN          269.7228\n",
       "1576  2.614572e+06  1.266000e+06        NaN          269.7057\n",
       "1577  2.614571e+06  1.265997e+06        NaN          269.7057\n",
       "1578  2.614570e+06  1.265994e+06        NaN          269.7682\n",
       "1579  2.614569e+06  1.265991e+06        NaN          269.6732\n",
       "1580  2.614568e+06  1.265988e+06        NaN          269.6079\n",
       "1581  2.614567e+06  1.265985e+06        NaN          269.6079\n",
       "1582  2.614566e+06  1.265983e+06        NaN          269.6148\n",
       "1583  2.614565e+06  1.265980e+06        NaN          269.6337\n",
       "1584  2.614564e+06  1.265977e+06        NaN          269.5825\n",
       "1585  2.614563e+06  1.265974e+06        NaN          269.5660\n",
       "1586  2.614562e+06  1.265971e+06        NaN          269.5660\n",
       "1587  2.614561e+06  1.265968e+06        NaN          269.5902\n",
       "1588  2.614560e+06  1.265966e+06        NaN          269.5902\n",
       "1589  2.614559e+06  1.265963e+06        NaN          269.3810\n",
       "1590  2.614559e+06  1.265961e+06        NaN          269.3810\n",
       "1591  2.612476e+06  1.268943e+06        NaN          262.9595\n",
       "1592  2.612491e+06  1.268917e+06        NaN          262.8025\n",
       "\n",
       "[1593 rows x 4 columns]"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Alignment_DTM_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# ----------------------------------------------------------------------------------------------------------------\n",
    "# prepare for join of grass results using pandas\n",
    "# result: alignment_df, Alignment_DTM_df_sel, Alignment_RockSurface_df_sel\n",
    "# ----------------------------------------------------------------------------------------------------------------\n",
    "\n",
    "# prepare alignment_df\n",
    "    # check:  alignment_df.head()\n",
    "alignment_df = alignment_df.loc[:,[\"Station\",\"Easting\", \"Northing\", \"Elevation\", \"StationReal\"]]\n",
    "    # check:  alignment_df.head()\n",
    "\n",
    "# prepare Alignment_DTM_df_sel\n",
    "    # check:  Alignment_DTM_df.head()\n",
    "Alignment_DTM_df_coleqtmp = [col for col in Alignment_DTM_df.columns if 'tmp' in col]\n",
    "if len(Alignment_DTM_df_coleqtmp) != 1:\n",
    "    print \"Extraction of DTM col=tmp did not work properly. Please check\"\n",
    "    exit()\n",
    "Alignment_DTM_df_rename = Alignment_DTM_df.rename(\n",
    "    columns= {Alignment_DTM_df_coleqtmp[0]: \"DTM\"})\n",
    "Alignment_DTM_df_sel = Alignment_DTM_df_rename.loc[:,[\"easting\", \"northing\", \"DTM\"]]\n",
    "    # check:  Alignment_RockSurface_df.head()\n",
    "\n",
    "# prepare Alignment_RockSurface_df_coleqtmp\n",
    "Alignment_RockSurface_df_coleqtmp = [col for col in Alignment_RockSurface_df.columns if 'tmp' in col]\n",
    "if len(Alignment_RockSurface_df_coleqtmp) != 1:\n",
    "    print \"Extraction of RockSurface_csv_coleqtmp col=tmp did not work properly. Please check\"\n",
    "    exit()\n",
    "Alignment_RockSurface_df_rename = Alignment_RockSurface_df.rename(\n",
    "    columns= {Alignment_RockSurface_df_coleqtmp[0]: \"RockSurface\"})\n",
    "Alignment_RockSurface_df_sel = Alignment_RockSurface_df_rename.loc[\n",
    "    :,[\"easting\", \"northing\", \"RockSurface\"]]  \n",
    "    # check:  Alignment_RockSurface_df_sel.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "merge_final\n"
     ]
    }
   ],
   "source": [
    "# ----------------------------------------------------------------------------------------------------------------\n",
    "# join grass results using Panda\n",
    "#    merge handles floats as keys inconsistently, round df's to three decimals before merge \n",
    "# result: merge_final\n",
    "print 'merge_final'\n",
    "# ----------------------------------------------------------------------------------------------------------------\n",
    "\n",
    "alignment_df = alignment_df.round(decimals=3)\n",
    "Alignment_DTM_df_sel = Alignment_DTM_df_sel.round(decimals=3)\n",
    "Alignment_RockSurface_df_sel = Alignment_RockSurface_df_sel.round(decimals=3)\n",
    "\n",
    "# merge DTM to Alignment\n",
    "merge_Alignment_DTM= pd.merge(left= alignment_df, right = Alignment_DTM_df_sel, \n",
    "                 left_on = [\"Easting\",\"Northing\"], \n",
    "                 right_on = [\"easting\",\"northing\"], how = \"left\")\n",
    "\n",
    "# merge RockSurface to Alignment_DTM\n",
    "merge_final = pd.merge(merge_Alignment_DTM, Alignment_RockSurface_df_sel, \n",
    "                 left_on = [\"Easting\",\"Northing\"], \n",
    "                 right_on = [\"easting\",\"northing\"])\n",
    "    # check:  merge_final.head()\n",
    "    # check:  merge_final.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cleaning up merge\n"
     ]
    }
   ],
   "source": [
    "# ----------------------------------------------------------------------------------------------------------------\n",
    "# clean up merge_final\n",
    "# result: TunnExcvDF\n",
    "print 'cleaning up merge'\n",
    "# ----------------------------------------------------------------------------------------------------------------\n",
    "\n",
    "TunnExcvDF = merge_final.loc[:,[\"Station\",\"Easting\", \"Northing\", \"Elevation\", \"DTM\", \"RockSurface\",\n",
    "                               \"StationReal\"]]\n",
    "    # check:  TunnExcvDF.head()\n",
    "    # check:  list(TunnExcvDF)\n",
    "# sort by Station\n",
    "#TunnExcvDF = TunnExcvDF.sort(['StationReal'], ascending=[1])  #sort depreacted\n",
    "TunnExcvDF = TunnExcvDF.sort_values(['StationReal'], ascending=[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# ----------------------------------------------------------------------------------------------------------------\n",
    "# calculate difference height rocksurface and tunnel axis\n",
    "# result: TunnExcvDF['RockCover']\n",
    "# ----------------------------------------------------------------------------------------------------------------\n",
    "\n",
    "TunnExcvDF['RockCover'] = TunnExcvDF.RockSurface - TunnExcvDF.Elevation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "working on WBS etc\n"
     ]
    }
   ],
   "source": [
    "# ----------------------------------------------------------------------------------------------------------------\n",
    "# assign WBS, WorkType, Excavation Type, Profile Type, Section Area from TunnelLayoutDarta\n",
    "# result: TunnExcvDF[\"WBScode\"], TunnExcvDF[\"WorkType\"], TunnExcvDF[\"ExcavationType\"], TunnExcvDF[\"ProfileType\"]\n",
    "#         TunnExcvDF[\"SectionArea\"], TunnExcvDF[\"Description\"]\n",
    "print \"working on WBS etc\"\n",
    "# ----------------------------------------------------------------------------------------------------------------\n",
    "\n",
    "TunnExcvDF[\"WBScode\"] = np.nan\n",
    "TunnExcvDF[\"WorkType\"] = np.nan\n",
    "TunnExcvDF[\"ExcavationType\"] = np.nan\n",
    "TunnExcvDF[\"ProfileType\"] = np.nan\n",
    "TunnExcvDF[\"SectionArea\"] = np.nan\n",
    "TunnExcvDF[\"Description\"] = np.nan\n",
    "TunnExcvDF[\"Unit\"] = volume_unit\n",
    "\n",
    "for n in range(0, len(layout_StationReal_list)):\n",
    "    nn = n+1 \n",
    "    if n == len(layout_StationReal_list) -1:\n",
    "        layout_StationReal_list.append(1e12)\n",
    "    TunnExcvDF.loc[(TunnExcvDF[\"StationReal\"] >= layout_StationReal_list[n])\n",
    "        & (TunnExcvDF[\"StationReal\"] < layout_StationReal_list[nn]), \"WBScode\"] \\\n",
    "        = layout_df[\"WBScode\"].tolist()[n]\n",
    "    TunnExcvDF.loc[(TunnExcvDF[\"StationReal\"] >= layout_StationReal_list[n])\n",
    "        & (TunnExcvDF[\"StationReal\"] < layout_StationReal_list[nn]), \"WorkType\"] \\\n",
    "        = layout_df[\"WorkType\"].tolist()[n]\n",
    "    TunnExcvDF.loc[(TunnExcvDF[\"StationReal\"] >= layout_StationReal_list[n])\n",
    "        & (TunnExcvDF[\"StationReal\"] < layout_StationReal_list[nn]), \"ExcavationType\"] \\\n",
    "        = layout_df[\"ExcavationType\"].tolist()[n]\n",
    "    TunnExcvDF.loc[(TunnExcvDF[\"StationReal\"] >= layout_StationReal_list[n])\n",
    "        & (TunnExcvDF[\"StationReal\"] < layout_StationReal_list[nn]), \"ProfileType\"] \\\n",
    "        = layout_df[\"ProfileType\"].tolist()[n]\n",
    "    TunnExcvDF.loc[(TunnExcvDF[\"StationReal\"] >= layout_StationReal_list[n])\n",
    "        & (TunnExcvDF[\"StationReal\"] < layout_StationReal_list[nn]), \"SectionArea\"] \\\n",
    "        = layout_df[\"SectionArea\"].tolist()[n]\n",
    "    TunnExcvDF.loc[(TunnExcvDF[\"StationReal\"] >= layout_StationReal_list[n])\n",
    "        & (TunnExcvDF[\"StationReal\"] < layout_StationReal_list[nn]), \"Description\"] \\\n",
    "        = layout_df[\"Description\"].tolist()[n]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "calculating BoreClass, SupportClass and DisposalClass\n",
      "BC3    805\n",
      "BC2    188\n",
      "BC1     60\n",
      "Name: BoreClass, dtype: int64\n",
      "TBM    1053\n",
      "MUL      51\n",
      "Name: ExcavationType, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# ----------------------------------------------------------------------------------------------------------------\n",
    "# calculate \"BoreClass\", \"SupportClass\" and \"DisposalClass\"\n",
    "# result: TunnExcvDF[\"BoreClass\"], TunnExcvDF[\"SupportClass\"], TunnExcvDF[\"DisposalClass\"]\n",
    "print 'calculating BoreClass, SupportClass and DisposalClass'\n",
    "# ----------------------------------------------------------------------------------------------------------------\n",
    "\n",
    "TunnExcvDF[\"BoreClass\"]= np.nan\n",
    "TunnExcvDF[\"SupportClass\"]= np.nan\n",
    "TunnExcvDF[\"DisposalClass\"]= np.nan\n",
    "\n",
    "# instantiate an instance of BoreClass\n",
    "bore_class=BoreClass()\n",
    "# call bore_class methods for BC1, BC2, BC3\n",
    "bore_class.bc1()\n",
    "bore_class.bc2()\n",
    "bore_class.bc3()\n",
    "print TunnExcvDF[\"BoreClass\"].value_counts()  # equals 805+188+60 for Ostroehre\n",
    "print TunnExcvDF[\"ExcavationType\"].value_counts() \n",
    "\n",
    "# Support Class                                                             # JK ToDo: define SC's as Class\n",
    "#  SCT\n",
    "TunnExcvDF.loc[(TunnExcvDF[\"ExcavationType\"] == \"TBM\"), \\\n",
    "    \"SupportClass\"] = \"SCT\"\n",
    "#  SC5\n",
    "TunnExcvDF.loc[(TunnExcvDF[\"ExcavationType\"] == \"MUL\"), \\\n",
    "    \"SupportClass\"] = \"SC5\"\n",
    "# check: TunnExcvDF[\"SupportClass\"].value_counts()\n",
    "# check: TunnExcvDF[\"ExcavationType\"].value_counts() \n",
    "\n",
    "# Disposal Class                                                            # JK ToDo: define MC's as Class\n",
    "#  MC5\n",
    "TunnExcvDF.loc[(TunnExcvDF[\"BoreClass\"] ==  \"BC1\") | (TunnExcvDF[\"BoreClass\"] == \"BC2\"), \\\n",
    "    \"DisposalClass\"] = \"MC5\"\n",
    "#  MC3\n",
    "TunnExcvDF.loc[(TunnExcvDF[\"BoreClass\"] ==  \"BC3\"), \\\n",
    "    \"DisposalClass\"] = \"MC3\"\n",
    "#  MC2\n",
    "TunnExcvDF.loc[(TunnExcvDF[\"ExcavationType\"] == \"MUL\"), \\\n",
    "    \"DisposalClass\"] = \"MC2\"\n",
    "# check: TunnExcvDF[\"DisposalClass\"].value_counts()\n",
    "# check: TunnExcvDF[\"ExcavationType\"].value_counts() # 805+248\n",
    "# check:\n",
    "#     print TunnExcvDF.loc[:,[\"Station\",\"ExcavationType\",\"BoreClass\",\"SupportClass\",\"DisposalClass\"]].to_string()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "calcuating excavation volume\n"
     ]
    }
   ],
   "source": [
    "# ----------------------------------------------------------------------------------------------------------------\n",
    "# calculate excavation volume of tunnel between two axis points\n",
    "print 'calcuating excavation volume'\n",
    "# ----------------------------------------------------------------------------------------------------------------\n",
    "\n",
    "# initialize interval length (StationInterval field)\n",
    "TunnExcvDF[\"StationInterval\"] = np.nan\n",
    "TunnExcvDF[\"ExcavationVolume\"] = np.nan\n",
    "\n",
    "# Calculate \"StationInterval\", \"Area1_mean_dist\" and \"Area2_mean_dist\"\n",
    "n = 0\n",
    "\n",
    "# use .iat instead of .iloc to return scalar values (*1000 faster)\n",
    "# LayoutData must show missing data as NaN (None is read as string value)\n",
    "for i in range(len(TunnExcvDF.index) -1):\n",
    "    nn= n+1\n",
    "    TunnExcvDF[\"StationInterval\"].iat[n] = ((TunnExcvDF[\"Easting\"].iat[nn] -TunnExcvDF[\"Easting\"].iat[n])**2 \n",
    "        +(TunnExcvDF[\"Northing\"].iat[nn] -TunnExcvDF[\"Northing\"].iat[n])**2 \n",
    "        +(TunnExcvDF[\"Elevation\"].iat[nn] -TunnExcvDF[\"Elevation\"].iat[n])**2 )**(0.5)\n",
    "    TunnExcvDF[\"ExcavationVolume\"].iat[n] = TunnExcvDF[\"SectionArea\"].iat[n] * TunnExcvDF[\"StationInterval\"].iat[n]\n",
    "    n = n+1\n",
    "# check:\n",
    "#    print TunnExcvDF.loc[:,[\"Station\",\"ExcavationType\",\"StationInterval\",\"ExcavationVolume\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "calcuating disposal volume\n"
     ]
    }
   ],
   "source": [
    "# ----------------------------------------------------------------------------------------------------------------\n",
    "# calculate disposal volume of tunnel between two axis points\n",
    "print 'calcuating disposal volume'\n",
    "# result: file TunnelExcavationData as .csv)\n",
    "# ----------------------------------------------------------------------------------------------------------------\n",
    "\n",
    "# mv to beginning of file                                                        JK ToDo\n",
    "def disposal_volume(ExcavationVolume, DisposalClass):\n",
    "    #calculate Disposal Volumes based on Disposal Class\n",
    "    DisposalVolume=np.nan\n",
    "    if DisposalClass==\"MC2\":\n",
    "        DisposalVolume=1.3*ExcavationVolume\n",
    "    elif DisposalClass==\"MC3\":\n",
    "        DisposalVolume=1.5*ExcavationVolume \n",
    "    elif DisposalClass==\"MC5\":\n",
    "        DisposalVolume=1.3*ExcavationVolume\n",
    "    #else:\n",
    "        #print \"unknown disposal class\"\n",
    "    return DisposalVolume\n",
    "\n",
    "TunnExcvDF[\"DisposalVolume\"] = np.nan\n",
    "n = 0\n",
    "for i in range(len(TunnExcvDF.index) -1):\n",
    "    TunnExcvDF[\"DisposalVolume\"].iat[n] = (\n",
    "        disposal_volume(TunnExcvDF[\"ExcavationVolume\"].iat[n],TunnExcvDF[\"DisposalClass\"].iat[n]) )\n",
    "    n = n+1\n",
    "# check:\n",
    "#  print TunnExcvDF.loc[:,[\"Station\",\"DisposalType\",\"ExcavationVolume\",\"DisposalVolume\"]]\n",
    "\n",
    "TunnExcvDF.to_csv(TunnelExcavationData, sep=\",\", na_rep=\"NaN\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "creating BoQ\n",
      "111a UEX MUL 205+490 205+520 SC5 3604.433653907168 m3\n",
      "111a UEX MUL 205+490 205+520 MC2 4685.763750079318 m3\n",
      "111b UEX TBM 205+520 208+620 BC1 23243.5981961306 m3\n",
      "111b UEX TBM 205+520 208+620 BC2 72922.24437796093 m3\n",
      "111b UEX TBM 205+520 208+620 BC3 315378.98568712483 m3\n",
      "111b UEX TBM 205+520 208+620 SCT 411544.82826121635 m3\n",
      "111b UEX TBM 205+520 208+620 MC5 125015.595346319 m3\n",
      "111b UEX TBM 205+520 208+620 MC3 473068.4785306873 m3\n",
      "111c UEX MUL 208+620 208+740 SC5 14405.445997409155 m3\n",
      "111c UEX MUL 208+620 208+740 MC2 18727.0797966319 m3\n"
     ]
    }
   ],
   "source": [
    "# ----------------------------------------------------------------------------------------------------------------\n",
    "# create BoQ and write to file\n",
    "# results: BoQ_df and BoQ as .csv\n",
    "print 'creating BoQ'\n",
    "# ----------------------------------------------------------------------------------------------------------------\n",
    "# replace print with write to file                                             ToDo JK\n",
    "\n",
    "# initialize a BoQ_list\n",
    "BoQ_list_headers= [\"WBS\",\"WorkType\",\"ExcavationType\",\"StationFrom\",\"StationTo\",\"PayItem\",\"Quantity\",\"Unit\"]\n",
    "BoQ_list_values=[]\n",
    "\n",
    "# find combinations of WBScode, ExcavationType and [BoreClass | Support Class | Disposal Class that exist\n",
    "# calculate excavation volume for each combination\n",
    "for i in TunnExcvDF[\"WBScode\"].unique():\n",
    "    for j in TunnExcvDF[\"ExcavationType\"].unique():\n",
    "        if \n",
    "             & (TunnExcvDF[\"ExcavationType\"] == j)).any():\n",
    "            work_type = (TunnExcvDF.loc[\n",
    "                ((TunnExcvDF[\"WBScode\"] == i)\n",
    "                & (TunnExcvDF[\"ExcavationType\"] == j)),\"WorkType\"]).unique()[0]\n",
    "        for k in TunnExcvDF[\"BoreClass\"].unique():\n",
    "            # if DF record with i, j, k (as Bore Class) exists:\n",
    "            if ((TunnExcvDF[\"WBScode\"] == i)\n",
    "                & (TunnExcvDF[\"ExcavationType\"] == j)\n",
    "                & (TunnExcvDF[\"BoreClass\"] == k)).any():\n",
    "                start_station = min(TunnExcvDF.loc[\n",
    "                    ((TunnExcvDF[\"WBScode\"] == i)\n",
    "                     & (TunnExcvDF[\"ExcavationType\"] == j)),\"Station\"])\n",
    "                end_station = max(TunnExcvDF.loc[\n",
    "                    ((TunnExcvDF[\"WBScode\"] == i)\n",
    "                     & (TunnExcvDF[\"ExcavationType\"] == j)),\"Station\"])\n",
    "                #need 'Station +1' because we are going From: To: along alignment\n",
    "                #TunnExcvDF.loc[(TunnExcvDF[\"Station\"] == end_station),\"Station\"].values[0]    ToDo Note JK\n",
    "                end_station_index=TunnExcvDF.index.get_loc(\n",
    "                    TunnExcvDF.loc[(TunnExcvDF[\"Station\"] == end_station),\"Station\"].index[0]) +1\n",
    "                end_station=TunnExcvDF.iloc[end_station_index,TunnExcvDF.columns.get_loc(\"Station\")]\n",
    "                volume_sum=TunnExcvDF.loc[\n",
    "                    ((TunnExcvDF[\"WBScode\"] == i)\n",
    "                     & (TunnExcvDF[\"ExcavationType\"] == j)\n",
    "                     & (TunnExcvDF[\"BoreClass\"] == k)),\"ExcavationVolume\"].sum()\n",
    "                BoQ_list_values.append((i,work_type,j,start_station,end_station,k,volume_sum,volume_unit))\n",
    "                print i, work_type, j, start_station, end_station, k, volume_sum, volume_unit\n",
    "        for k in TunnExcvDF[\"SupportClass\"].unique():\n",
    "            # if DF record with i, j, k (as Support Class) exists:\n",
    "            if ((TunnExcvDF[\"WBScode\"] == i)\n",
    "                & (TunnExcvDF[\"ExcavationType\"] == j)\n",
    "                & (TunnExcvDF[\"SupportClass\"] == k)).any():                    \n",
    "                start_station = min(TunnExcvDF.loc[\n",
    "                    ((TunnExcvDF[\"WBScode\"] == i)\n",
    "                     & (TunnExcvDF[\"ExcavationType\"] == j)),\"Station\"])\n",
    "                end_station = max(TunnExcvDF.loc[\n",
    "                    ((TunnExcvDF[\"WBScode\"] == i)\n",
    "                     & (TunnExcvDF[\"ExcavationType\"] == j)),\"Station\"])\n",
    "                end_station_index=TunnExcvDF.index.get_loc(\n",
    "                    TunnExcvDF.loc[(TunnExcvDF[\"Station\"] == end_station),\"Station\"].index[0]) +1\n",
    "                end_station=TunnExcvDF.iloc[end_station_index,TunnExcvDF.columns.get_loc(\"Station\")]\n",
    "                volume_sum=TunnExcvDF.loc[\n",
    "                    ((TunnExcvDF[\"WBScode\"] == i)\n",
    "                     & (TunnExcvDF[\"ExcavationType\"] == j)\n",
    "                     & (TunnExcvDF[\"SupportClass\"] == k)),\"ExcavationVolume\"].sum()\n",
    "                BoQ_list_values.append((i,work_type,j,start_station,end_station,k,volume_sum,volume_unit))\n",
    "                print i, work_type, j, start_station, end_station, k, volume_sum, volume_unit\n",
    "        for k in TunnExcvDF[\"DisposalClass\"].unique():\n",
    "            # if DF record with i, j, k (as Support Class) exists:\n",
    "            if ((TunnExcvDF[\"WBScode\"] == i)\n",
    "                & (TunnExcvDF[\"ExcavationType\"] == j)\n",
    "                & (TunnExcvDF[\"DisposalClass\"] == k)).any():                    \n",
    "                start_station = min(TunnExcvDF.loc[\n",
    "                    ((TunnExcvDF[\"WBScode\"] == i)\n",
    "                     & (TunnExcvDF[\"ExcavationType\"] == j)),\"Station\"])\n",
    "                end_station = max(TunnExcvDF.loc[\n",
    "                    ((TunnExcvDF[\"WBScode\"] == i)\n",
    "                     & (TunnExcvDF[\"ExcavationType\"] == j)),\"Station\"])\n",
    "                end_station_index=TunnExcvDF.index.get_loc(\n",
    "                    TunnExcvDF.loc[(TunnExcvDF[\"Station\"] == end_station),\"Station\"].index[0]) +1\n",
    "                end_station=TunnExcvDF.iloc[end_station_index,TunnExcvDF.columns.get_loc(\"Station\")]\n",
    "                volume_sum=TunnExcvDF.loc[\n",
    "                    ((TunnExcvDF[\"WBScode\"] == i)\n",
    "                     & (TunnExcvDF[\"ExcavationType\"] == j)\n",
    "                     & (TunnExcvDF[\"DisposalClass\"] == k)),\"DisposalVolume\"].sum()\n",
    "                BoQ_list_values.append((i,work_type,j,start_station,end_station,k,volume_sum,volume_unit))\n",
    "                print i, work_type, j, start_station, end_station, k, volume_sum, volume_unit\n",
    "# check:\n",
    "#print TunnExcvDF.loc[TunnExcvDF[\"ExcavationType\"] == \"TBM\", \"ExcavationVolume\"].sum()\n",
    "#print TunnExcvDF.loc[TunnExcvDF[\"ExcavationType\"] == \"TBM\", \"DisposalVolume\"].sum()\n",
    "\n",
    "BoQ_df =  pd.DataFrame(BoQ_list_values, columns=BoQ_list_headers).round(decimals=3)\n",
    "BoQ_df.to_csv(BoQ, sep=\",\", na_rep=\"NaN\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
